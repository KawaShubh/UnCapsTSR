{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05b3f7f6-03f9-49c1-99ea-515dd34750be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from basicsr.utils.registry import ARCH_REGISTRY\n",
    "from basicsr.archs.arch_util import to_2tuple, trunc_normal_\n",
    "import math\n",
    "from einops import rearrange\n",
    "####################\n",
    "# Basic blocks\n",
    "####################\n",
    "\n",
    "\n",
    "def act(act_type, inplace=True, neg_slope=0.2, n_selu=1):\n",
    "    # helper selecting activation\n",
    "    # neg_slope: for selu and init of selu\n",
    "    # n_selu: for p_relu num_parameters\n",
    "    act_type = act_type.lower()\n",
    "    if act_type == 'relu':\n",
    "        layer = nn.ReLU()\n",
    "    elif act_type == 'leakyrelu':\n",
    "        layer = nn.LeakyReLU(0.2,inplace)\n",
    "    elif act_type == 'prelu':\n",
    "        layer = nn.PReLU()\n",
    "    elif act_type == 'sigm':\n",
    "        layer = nn.Sigmoid()\n",
    "    elif act_type == 'elu':\n",
    "        layer = nn.ELU()\n",
    "    else:\n",
    "        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n",
    "    return layer\n",
    "\n",
    "\n",
    "def norm(norm_type, nc):\n",
    "    # helper selecting normalization layer\n",
    "    norm_type = norm_type.lower()\n",
    "    if norm_type == 'batch':\n",
    "        layer = nn.BatchNorm2d(nc, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n",
    "    return layer\n",
    "\n",
    "\n",
    "def pad(pad_type, padding):\n",
    "    # helper selecting padding layer\n",
    "    # if padding is 'zero', do by conv layers\n",
    "    pad_type = pad_type.lower()\n",
    "    if padding == 0:\n",
    "        return None\n",
    "    if pad_type == 'reflect':\n",
    "        layer = nn.ReflectionPad2d(padding)\n",
    "    elif pad_type == 'replicate':\n",
    "        layer = nn.ReplicationPad2d(padding)\n",
    "    else:\n",
    "        raise NotImplementedError('padding layer [{:s}] is not implemented'.format(pad_type))\n",
    "    return layer\n",
    "\n",
    "def get_valid_padding(kernel_size, dilation):\n",
    "    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    padding = (kernel_size - 1) // 2\n",
    "    return padding\n",
    "\n",
    "\n",
    "class ConcatBlock(nn.Module):\n",
    "    # Concat the output of a submodule to its input\n",
    "    def __init__(self, submodule):\n",
    "        super(ConcatBlock, self).__init__()\n",
    "        self.sub = submodule\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat((x, self.sub(x)), dim=1)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        tmpstr = 'Identity .. \\n|'\n",
    "        modstr = self.sub.__repr__().replace('\\n', '\\n|')\n",
    "        tmpstr = tmpstr + modstr\n",
    "        return tmpstr\n",
    "\n",
    "\n",
    "class ShortcutBlock(nn.Module):\n",
    "    #Elementwise sum the output of a submodule to its input\n",
    "    def __init__(self, submodule):\n",
    "        super(ShortcutBlock, self).__init__()\n",
    "        self.sub = submodule\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x + self.sub(x)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        tmpstr = 'Identity + \\n|'\n",
    "        modstr = self.sub.__repr__().replace('\\n', '\\n|')\n",
    "        tmpstr = tmpstr + modstr\n",
    "        return tmpstr\n",
    "\n",
    "\n",
    "def sequential(*args):\n",
    "    # Flatten Sequential. It unwraps nn.Sequential.\n",
    "    if len(args) == 1:\n",
    "        if isinstance(args[0], OrderedDict):\n",
    "            raise NotImplementedError('sequential does not support OrderedDict input.')\n",
    "        return args[0]  # No sequential is needed.\n",
    "    modules = []\n",
    "    for module in args:\n",
    "        if isinstance(module, nn.Sequential):\n",
    "            for submodule in module.children():\n",
    "                modules.append(submodule)\n",
    "        elif isinstance(module, nn.Module):\n",
    "            modules.append(module)\n",
    "    return nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True, \\\n",
    "               pad_type='zero', norm_type=None, act_type='leakyrelu', mode='CNA'):\n",
    "    '''\n",
    "    Conv layer with padding, normalization, activation\n",
    "    mode: CNA --> Conv -> Norm -> Act\n",
    "        NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)\n",
    "    '''\n",
    "    assert mode in ['CNA', 'NAC', 'CNAC'], 'Wong conv mode [{:s}]'.format(mode)\n",
    "    padding = get_valid_padding(kernel_size, dilation)\n",
    "    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n",
    "    padding = padding if pad_type == 'zero' else 0\n",
    "\n",
    "    c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
    "            dilation=dilation, bias=bias, groups=groups)\n",
    "    a = act(act_type) if act_type else None\n",
    "    if 'CNA' in mode:\n",
    "        n = norm(norm_type, out_nc) if norm_type else None\n",
    "        return sequential(p, c, n, a)\n",
    "    elif mode == 'NAC':\n",
    "        if norm_type is None and act_type is not None:\n",
    "            a = act(act_type, inplace=False)\n",
    "            # Important!\n",
    "            # input----ReLU(inplace)----Conv--+----output\n",
    "            #        |________________________|\n",
    "            # inplace ReLU will modify the input, therefore wrong output\n",
    "        n = norm(norm_type, in_nc) if norm_type else None\n",
    "        return sequential(n, a, p, c)\n",
    "\n",
    "def trans_conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True, \\\n",
    "               pad_type='zero', norm_type=None, act_type='relu', mode='CNA'):\n",
    "    '''\n",
    "    Conv layer with padding, normalization, activation\n",
    "    mode: CNA --> Conv -> Norm -> Act\n",
    "        NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)\n",
    "    '''\n",
    "    assert mode in ['CNA', 'NAC', 'CNAC'], 'Wong conv mode [{:s}]'.format(mode)\n",
    "    padding = get_valid_padding(kernel_size, dilation)\n",
    "    #padding = 1\n",
    "    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n",
    "    padding = padding if pad_type == 'zero' else 0\n",
    "\n",
    "    c = nn.ConvTranspose2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \\\n",
    "            dilation=dilation, bias=bias, groups=groups,output_padding=1)\n",
    "    a = act(act_type) if act_type else None\n",
    "    if 'CNA' in mode:\n",
    "        n = norm(norm_type, out_nc) if norm_type else None\n",
    "        return sequential(p, c, n, a)\n",
    "    elif mode == 'NAC':\n",
    "        if norm_type is None and act_type is not None:\n",
    "            a = act(act_type, inplace=False)\n",
    "            # Important!\n",
    "            # input----ReLU(inplace)----Conv--+----output\n",
    "            #        |________________________|\n",
    "            # inplace ReLU will modify the input, therefore wrong output\n",
    "        n = norm(norm_type, in_nc) if norm_type else None\n",
    "        return sequential(n, a, p, c)\n",
    "\n",
    "####################\n",
    "# Useful blocks\n",
    "####################\n",
    "\n",
    "\n",
    "\n",
    "class UP_Sample(nn.Module):\n",
    "    def __init__(self,in_nc, nc, kernel_size=3, stride=1, bias=True, pad_type='zero', \\\n",
    "            act_type=None, mode='CNA',upscale_factor=2):\n",
    "        super(UP_Sample, self).__init__()\n",
    "        self.U1 = pixelshuffle_block(in_nc, nc, upscale_factor=upscale_factor, kernel_size=3, norm_type = 'batch')\n",
    "        self.co1 = conv_block(nc, 16, kernel_size=1, norm_type=None, act_type='leakyrelu', mode='CNA')\n",
    "        self.co2 = conv_block(16, 3, kernel_size=3, norm_type=None, act_type='leakyrelu', mode='CNA')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.U1(x)\n",
    "        return self.co2(self.co1(out1))\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, nf,reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Conv2d(nf,nf//reduction , kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nf//reduction, nf, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul(self.module(x))\n",
    "\n",
    "class HFM(nn.Module):\n",
    "    def __init__(self, k=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.k = k\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size = self.k, stride = self.k),\n",
    "            nn.Upsample(scale_factor = self.k, mode = 'nearest'),\n",
    "        )\n",
    "\n",
    "    def forward(self, tL):\n",
    "        assert tL.shape[2] % self.k == 0, 'h, w must divisible by k'\n",
    "        return tL - self.net(tL)        \n",
    "\n",
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, reScale=1, kernel_size=1, bias=True,norm_type='batch', act_type='leakyrelu'):\n",
    "        super(ResidualUnit,self).__init__()\n",
    "\n",
    "        self.reduction =conv_block(in_nc, in_nc//2, kernel_size=kernel_size, norm_type=norm_type, act_type=act_type)\n",
    " \n",
    "        self.expansion = conv_block(in_nc//2, in_nc, kernel_size=kernel_size, norm_type=norm_type, act_type=act_type)\n",
    "        self.lamRes = reScale\n",
    "        self.lamX = reScale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.reduction(x)\n",
    "        res1 = self.expansion(res)\n",
    "        res2 = self.lamRes * res1\n",
    "        x = self.lamX * x + res2\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ARFB(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, reScale=1,norm_type='batch', act_type='leakyrelu',kernel_size=1):\n",
    "        super().__init__()\n",
    "        self.RU1 = ResidualUnit(in_nc, out_nc, reScale)\n",
    "        self.RU2 = ResidualUnit(in_nc, out_nc, reScale)\n",
    "        self.conv1 =conv_block(2*in_nc, 2*in_nc, kernel_size=kernel_size, norm_type=norm_type, act_type=act_type) \n",
    "        self.conv3 =conv_block(2*in_nc, in_nc, kernel_size=3, norm_type=norm_type, act_type=act_type) \n",
    "        self.lamRes = reScale\n",
    "        self.lamX = reScale\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_ru1 = self.RU1(x)\n",
    "        x_ru2 = self.RU2(x_ru1)\n",
    "        x_ru = torch.cat((x_ru1, x_ru2), 1)\n",
    "        x_ru = self.conv1(x_ru)\n",
    "        x_ru = self.conv3(x_ru)\n",
    "        x_ru = self.lamRes * x_ru\n",
    "        x = x*self.lamX + x_ru\n",
    "        return x\n",
    "\n",
    "class high_block(nn.Module):\n",
    "    def __init__(self, in_nc, kernel_size=3,norm_type='batch', act_type='leakyrelu'):\n",
    "        super(high_block, self).__init__()\n",
    "\n",
    "        self.conv0 = conv_block(in_nc, in_nc, kernel_size=kernel_size, norm_type=norm_type, act_type=act_type)\n",
    "        self.conv1 = conv_block(in_nc, in_nc, kernel_size=kernel_size, norm_type=norm_type,act_type=act_type)\n",
    "        self.conv2 = conv_block(in_nc, in_nc, kernel_size=kernel_size, norm_type=norm_type, act_type=act_type)\n",
    "        #self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        #self.conv3 = conv_block(in_nc, 16, kernel_size=1, norm_type=None, act_type='prelu')\n",
    "        #self.conv4 = conv_block(16, in_nc, kernel_size=1, norm_type=None, act_type='sigm')\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1 = self.conv2(self.conv1(self.conv0(x)))\n",
    "        #m = self.conv4(self.conv3(self.gap(x1)))\n",
    "        #x2 = x1.mul(m)\n",
    "        return x1.mul(0.2) + x\n",
    "\n",
    "\n",
    "class BB(nn.Module) :\n",
    "  def __init__(self, nf) :\n",
    "    super(BB, self).__init__()\n",
    "    #self.k = k\n",
    "    # self.uk3_1 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 3)\n",
    "    # self.uk3_2 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 3)\n",
    "    # self.uk3_3 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 3)\n",
    "    #self.arf1=ARFB(in_nc=nf, out_nc=nf, reScale=1,norm_type='batch', act_type='leakyrelu',kernel_size=1)\n",
    "    self.hfm1=HFM(k=2)\n",
    "    self.uk3_1 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 3)\n",
    "    self.channel1=ChannelAttention(nf)\n",
    "    self.hfm4=HFM(k=2)\n",
    "    self.uk3_2 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 3)\n",
    "    self.channel2=ChannelAttention(nf)\n",
    "    self.hfm5=HFM(k=2)\n",
    "    self.uk3_3 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 3)\n",
    "    self.channel3=ChannelAttention(nf)\n",
    "    self.hfm6=HFM(k=2)\n",
    "    self.uk3_4 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 3)\n",
    "    self.channel10=ChannelAttention(nf)\n",
    "\n",
    "    \n",
    "    #self.arf2=ARFB(in_nc=nf, out_nc=nf, reScale=1,norm_type='batch', act_type='leakyrelu',kernel_size=1)\n",
    "    self.hfm2=HFM(k=2)\n",
    "    self.lk5_1 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 5)\n",
    "    self.channel4=ChannelAttention(nf)\n",
    "    self.hfm7=HFM(k=2)\n",
    "    self.lk5_2 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 5)\n",
    "    self.channel5=ChannelAttention(nf)\n",
    "    self.hfm8=HFM(k=2)\n",
    "    self.lk5_3 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 5)\n",
    "    self.channel6=ChannelAttention(nf)\n",
    "    self.hfm9=HFM(k=2)\n",
    "    self.lk5_4 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 5)\n",
    "    self.channel11=ChannelAttention(nf)\n",
    "\n",
    "    # self.lk7_1 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 7)\n",
    "    # self.lk7_2 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 7)\n",
    "    # self.lk7_3 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 7)\n",
    "    \n",
    "    #self.arf3=ARFB(in_nc=nf, out_nc=nf, reScale=1,norm_type='batch', act_type='leakyrelu',kernel_size=1)\n",
    "    self.hfm3=HFM(k=2)\n",
    "    self.lk1_1 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 1)\n",
    "    self.channel7=ChannelAttention(nf)\n",
    "    self.hfm10=HFM(k=2)\n",
    "    self.lk1_2 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 1)\n",
    "    self.channel8=ChannelAttention(nf)\n",
    "    self.hfm11=HFM(k=2)\n",
    "    self.lk1_3 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 1)\n",
    "    self.channel9=ChannelAttention(nf)\n",
    "    self.hfm12=HFM(k=2)\n",
    "    self.lk1_4 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 1)\n",
    "    self.channel12=ChannelAttention(nf)\n",
    "    #self.lk9_1 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 9)\n",
    "    #self.lk9_2 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 9)\n",
    "   # self.lk9_3 = conv_block(in_nc=nf, out_nc=nf, kernel_size= 9)\n",
    "    self.k1 = conv_block(in_nc=4*nf, out_nc=4*nf, kernel_size= 1)\n",
    "    self.k2 = conv_block(in_nc=4*nf, out_nc=nf, kernel_size= 3)\n",
    "\n",
    "    \n",
    "    # self.emha = EMHA(nf*k*k, splitfactors, heads)\n",
    "    # self.norm = nn.LayerNorm(nf*k*k)\n",
    "    # self.unFold = nn.Unfold(kernel_size=(k, k), padding=1)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    _, _, h, w = x.shape\n",
    "    \n",
    "\n",
    "    #upper path\n",
    "    #xarf1=self.arf1(x)\n",
    "    xu1_1hfm=self.hfm1(x)\n",
    "    xu1_1= self.uk3_1(xu1_1hfm)\n",
    "    xc1=self.channel1(xu1_1)\n",
    "    xc1=xc1+x\n",
    "    xc1=self.hfm4(xc1)\n",
    "    xu2_1= self.uk3_2(xc1)\n",
    "    xc2=self.channel2(xu2_1)\n",
    "    xc2=xc2+xc1\n",
    "    xc2=self.hfm5(xc2)\n",
    "    xu3_1= self.uk3_3(xc2)\n",
    "    xc3=self.channel3(xu3_1)\n",
    "    xc3=xc3+xc2\n",
    "    xc3=self.hfm6(xc3)\n",
    "    xu4_1=self.uk3_4(xc3)\n",
    "    xc10=self.channel10(xu4_1)\n",
    "\n",
    "\n",
    "    #xarf2=self.arf2(x)\n",
    "    xl0hfm=self.hfm2(x)\n",
    "    xl1= self.lk5_1(xl0hfm)\n",
    "    xc4=self.channel4(xl1)\n",
    "    xc4=xc4+x\n",
    "    xc4=self.hfm7(xc4)\n",
    "    xl2= self.lk5_2(xc4)\n",
    "    xc5=self.channel5(xl2)\n",
    "    xc5=xc5+xc4\n",
    "    xc5=self.hfm8(xc5)\n",
    "    xl3= self.lk5_3(xc5)\n",
    "    xc6=self.channel6(xl3)\n",
    "    xc6=xc6+xc5\n",
    "    xc6=self.hfm9(xc6)\n",
    "    xl4=self.lk5_4(xc6)\n",
    "    xc11=self.channel11(xl4)\n",
    "    \n",
    "\n",
    "    #lower part\n",
    "    #xarf3=self.arf3(x)\n",
    "    xu5_1hfm=self.hfm3(x)\n",
    "    xl5_1= self.lk1_1(xu5_1hfm)\n",
    "    xc7=self.channel7(xl5_1)\n",
    "    xc7=xc7+x\n",
    "    xc7=self.hfm10(xc7)\n",
    "    xl5_2= self.lk1_2(xc7)\n",
    "    xc8=self.channel8(xl5_2)\n",
    "    xc8=xc8+xc7\n",
    "    xc8=self.hfm11(xc8)\n",
    "    xl5_3= self.lk1_3(xc8)\n",
    "    xc9=self.channel9(xl5_3)\n",
    "    xc9=xc9+xc8\n",
    "    xc9=self.hfm12(xc9)\n",
    "    xl5_4=self.lk1_4(xc9)\n",
    "    xc12=self.channel12(xl5_4)\n",
    "\n",
    "    # #transformer\n",
    "    \n",
    "    out= torch.cat((xc10,xc11,xc12,x),1)\n",
    "    out1=self.k1(out)\n",
    "    out2=self.k2(out1)\n",
    "\n",
    "    return out2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "\n",
    "    def __init__(self, nf,act_type='relu'):\n",
    "        super(Residual,self).__init__()\n",
    "        #self.conv1 = conv_block(nf, nf, kernel_size=1, norm_type=None, act_type=act_type)\n",
    "        self.bb1 = BB(nf)\n",
    "        self.bb2 = BB(nf)\n",
    "        self.bb3 = BB(nf)\n",
    "        self.bb4 = BB(nf)\n",
    "        self.bb5 = BB(nf)\n",
    "        # self.k1 = conv_block(in_nc=5*nf, out_nc=nf, kernel_size= 3)\n",
    "        # self.k2 = conv_block(in_nc=2*nf, out_nc=nf, kernel_size= 3)\n",
    "        # self.k3 = conv_block(in_nc=3*nf, out_nc=nf, kernel_size= 3)\n",
    "        # self.k4 = conv_block(in_nc=4*nf, out_nc=nf, kernel_size= 3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x1= self.bb1(x)\n",
    "        xu1_1= self.bb1(x)\n",
    "        \n",
    "        xu2_1= self.bb2(xu1_1)\n",
    "        \n",
    "        # # # # out2 = self.channel2(xu2_2)\n",
    "        xu3_1= self.bb3(xu2_1)\n",
    "        \n",
    "        \n",
    "        # # # out3 = self.channel3(xu3_2)\n",
    "        xu3_3= self.bb4(xu3_1)\n",
    "       \n",
    "        # # # out4 = self.channel4(xu3_4)\n",
    "        xu3_5= self.bb5(xu3_3)\n",
    "       \n",
    "   \n",
    "        return xu3_5+x\n",
    "\n",
    "\n",
    "class EMHA(nn.Module):\n",
    "  \n",
    "    def __init__(self, in_nc, splitfactors=4, heads=8):\n",
    "        super().__init__()\n",
    "        dimHead = in_nc // (2*heads)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.splitfactors = splitfactors\n",
    "        self.scale = dimHead ** -0.5\n",
    "\n",
    "        self.reduction = nn.Conv1d(\n",
    "            in_channels=in_nc, out_channels=in_nc//2, kernel_size=1)\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.toQKV = nn.Linear(\n",
    "            in_nc // 2, in_nc // 2 * 3, bias=False)\n",
    "        self.expansion = nn.Conv1d(\n",
    "            in_channels=in_nc//2, out_channels=in_nc, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reduction(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        qkv = self.toQKV(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "        qs, ks, vs = map(lambda t: t.chunk(\n",
    "            self.splitfactors, dim=2), [q, k, v])\n",
    "\n",
    "        pool = []\n",
    "        for qi, ki, vi in zip(qs, ks, vs):\n",
    "            tmp = torch.matmul(qi, ki.transpose(-1, -2)) * self.scale\n",
    "            attn = self.attend(tmp)\n",
    "            out = torch.matmul(attn, vi)\n",
    "            out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "            pool.append(out)\n",
    "\n",
    "        out = torch.cat(tuple(pool), dim=1)\n",
    "        out = out.transpose(-1, -2)\n",
    "        out = self.expansion(out)\n",
    "        return out        \n",
    "\n",
    "#Residual transformer block\n",
    "class Transformer1(nn.Module):\n",
    "    def __init__(self, nf, splitfactors=4, heads=8, k=3):\n",
    "        super(Transformer1, self).__init__()\n",
    "        self.k = k\n",
    "        self.unFold1 = nn.Unfold(kernel_size=(k, k), padding=1)\n",
    "        self.emha1 = EMHA(nf*k*k, splitfactors, heads)\n",
    "        self.norm1 = nn.LayerNorm(nf*k*k)\n",
    "        self.conv1 = conv_block(nf, nf, kernel_size=3)\n",
    "        self.unFold2 = nn.Unfold(kernel_size=(k, k), padding=1)\n",
    "        self.norm2 = nn.LayerNorm(nf*k*k)\n",
    "        self.emha2 = EMHA(nf*k*k, splitfactors, heads)\n",
    "        self.conv2 = conv_block(nf, nf, kernel_size=3)\n",
    "        self.unFold3 = nn.Unfold(kernel_size=(k, k), padding=1)\n",
    "        self.norm3 = nn.LayerNorm(nf*k*k)\n",
    "        self.emha3 = EMHA(nf*k*k, splitfactors, heads)\n",
    "        self.conv3 = conv_block(nf, nf, kernel_size=3)\n",
    "    def forward(self,x):\n",
    "\n",
    "        _, _, h, w = x.shape\n",
    "        #rt1 = self.emha1(self.unFold1(x))\n",
    "        #residual block 1\n",
    "        rt1 = self.unFold1(x)\n",
    "        rt13 = rt1.transpose(-2, -1)\n",
    "        rt13 = self.norm1(rt13)\n",
    "        rt13 = rt13.transpose(-2, -1)\n",
    "        rt13 = self.emha1(rt13)+rt1\n",
    "        rt11 = F.fold(rt13, output_size=(h, w), kernel_size=(self.k, self.k), padding=(1, 1))\n",
    "        rt11 = rt11+x\n",
    "        #print(rt11.shape)\n",
    "        rt12 = self.conv1(rt11)\n",
    "        #residual block 2\n",
    "        rt2 = self.unFold2(rt12)\n",
    "        rt23 = rt2.transpose(-2, -1)\n",
    "        rt23 = self.norm2(rt23)\n",
    "        rt23 = rt23.transpose(-2, -1)\n",
    "        rt23 =self.emha2(rt23)+rt2\n",
    "        rt21 = F.fold(rt23, output_size=(h, w), kernel_size=(self.k, self.k), padding=(1, 1))\n",
    "        rt21 = rt21+rt12\n",
    "        #print(rt21.shape)\n",
    "        rt22 = self.conv2(rt21)\n",
    "        #residual block 3\n",
    "        rt3 = self.unFold3(rt22)\n",
    "        rt33 = rt3.transpose(-2, -1)\n",
    "        rt33 = self.norm3(rt33)\n",
    "        rt33 = rt33.transpose(-2, -1)\n",
    "        rt33 = self.emha3(rt33)+rt3\n",
    "        rt31 = F.fold(rt33, output_size=(h, w), kernel_size=(self.k, self.k), padding=(1, 1))\n",
    "        rt31 = rt31+rt22\n",
    "        #print(rt31.shape)\n",
    "        rt32 = self.conv3(rt31)\n",
    "        rt32 = rt32+x\n",
    "        return rt32\n",
    "\n",
    "\n",
    "class VIT(nn.Module):\n",
    "    def __init__(self, in_nc, nf, splitfactors=4, heads=8,norm_type='batch', act_type='elu'):\n",
    "        super(VIT, self).__init__()\n",
    "        self.FUP = nn.Upsample(scale_factor=4, mode='bicubic')\n",
    "\n",
    "        \n",
    "        #self.conv1 = conv_block(nf,nf,kernel_size=3,norm_type=norm_type,act_type=act_type)\n",
    "        self.sfe = conv_block(in_nc, nf, kernel_size=3, act_type='elu')\n",
    "        self.sfe0 = base_block(nf, kernel_size=3, norm_type=norm_type,act_type=act_type)\n",
    "        self.RT1 = Transformer1(nf, splitfactors, heads)\n",
    "        self.RT2 = Transformer1(nf, splitfactors, heads)\n",
    "        self.RT3 = Transformer1(nf, splitfactors, heads)\n",
    "        self.RT4 = Transformer1(nf, splitfactors, heads)\n",
    "        self.RT5 = Transformer1(nf, splitfactors, heads)\n",
    "      \n",
    "        self.c1 = conv_block(nf, nf, kernel_size=3, act_type=None,norm_type=None)\n",
    "        self.up = UP_Sample(nf,nf, kernel_size=3, act_type='elu',upscale_factor=4)\n",
    "        #self.up1 = upconv_blcok(nf, nf, upscale_factor=4)\n",
    "        self.c2 = conv_block(nf, 3, kernel_size=3)\n",
    "    def forward(self, x):\n",
    "       # xconv=self.conv1(x)\n",
    "        \n",
    "        \n",
    "        x1 = self.sfe(x)\n",
    "        xFUP = self.FUP(x)\n",
    "        xbase=self.sfe0(x1)\n",
    "        x2 = self.RT1(xbase)\n",
    "        x3 = self.RT2(x2)\n",
    "        x4 = self.RT3(x3)\n",
    "        x5 = self.RT4(x4)\n",
    "        x6 = self.RT5(x5)\n",
    "      \n",
    "        xa = self.c1(x6)\n",
    "        xa = xa+xbase\n",
    "        xu1 = self.up(xa)\n",
    "        xout = xu1+xFUP\n",
    "        return xout\n",
    "\n",
    "\n",
    "\n",
    "class high_low_network(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc,nb=20, nf=32,splitfactors=4, heads=8, kernel_size=3,norm_type='batch', act_type='leakyrelu'):\n",
    "        super(high_low_network, self).__init__()\n",
    "\n",
    "        self.FUP = nn.Upsample(scale_factor=4, mode='bicubic')\n",
    "\n",
    "        \n",
    "        #self.conv1 = conv_block(nf,nf,kernel_size=3,norm_type=norm_type,act_type=act_type)\n",
    "        self.sfe = conv_block(in_nc, nf, kernel_size=3, act_type='leakyrelu')\n",
    "        #self.hfm=HFM(k=2)\n",
    "        self.sfe0 = Residual(nf)\n",
    "        #self.RT1 = Transformer1(nf, splitfactors, heads)\n",
    "        #self.RT2 = Transformer1(nf, splitfactors, heads)\n",
    "        #self.RT3 = Transformer1(nf, splitfactors, heads)\n",
    "        #self.RT4 = Transformer1(nf, splitfactors, heads)\n",
    "        #self.RT5 = Transformer1(nf, splitfactors, heads)\n",
    "        #self.RT6 = Transformer1(nf, splitfactors, heads)\n",
    "      \n",
    "        self.c1 = conv_block(nf, nf, kernel_size=3, act_type=None,norm_type=None)\n",
    "        self.up = UP_Sample(nf,nf, kernel_size=3, act_type='leakyrelu',upscale_factor=4)\n",
    "        #self.up1 = upconv_blcok(nf, nf, upscale_factor=4)\n",
    "        self.c2 = conv_block(nf, 3, kernel_size=3)\n",
    "        #self.CA= ChannelAttention(nf)\n",
    "    def forward(self, x):\n",
    "       # xconv=self.conv1(x)\n",
    "        \n",
    "        \n",
    "        x1 = self.sfe(x)\n",
    "        #xhfm=self.hfm(x1)\n",
    "        #x8=self.CA(x1)\n",
    "        xFUP = self.FUP(x)\n",
    "        xbase=self.sfe0(x1)\n",
    "        #x2 = self.RT1(xbase)\n",
    "        #x3 = self.RT2(x2)\n",
    "        #x4 = self.RT3(x3)\n",
    "        #x5 = self.RT4(x4)\n",
    "        #x6 = self.RT5(x5)\n",
    "        #x7 = self.RT6(x6)\n",
    "      \n",
    "        xa = self.c1(xbase)\n",
    "        xu1 = self.up(xa)\n",
    "        xout = xu1+xFUP\n",
    "        return xout\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "\n",
    "    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention used in RCAN.\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "        squeeze_factor (int): Channel squeeze factor. Default: 16.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feat, squeeze_factor=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(num_feat, num_feat // squeeze_factor, 1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(num_feat // squeeze_factor, num_feat, 1, padding=0),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class CAB(nn.Module):\n",
    "\n",
    "    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):\n",
    "        super(CAB, self).__init__()\n",
    "\n",
    "        self.cab = nn.Sequential(\n",
    "            nn.Conv2d(num_feat, num_feat // compress_ratio, 3, 1, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(num_feat // compress_ratio, num_feat, 3, 1, 1),\n",
    "            ChannelAttention(num_feat, squeeze_factor)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cab(x)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (b, h, w, c)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*b, window_size, window_size, c)\n",
    "    \"\"\"\n",
    "    b, h, w, c = x.shape\n",
    "    x = x.view(b, h // window_size, window_size, w // window_size, window_size, c)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, c)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*b, window_size, window_size, c)\n",
    "        window_size (int): Window size\n",
    "        h (int): Height of image\n",
    "        w (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (b, h, w, c)\n",
    "    \"\"\"\n",
    "    b = int(windows.shape[0] / (h * w / window_size / window_size))\n",
    "    x = windows.view(b, h // window_size, w // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, rpi, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*b, n, c)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        b_, n, c = x.shape\n",
    "        print(x.shape)\n",
    "        qkv = self.qkv(x).reshape(b_, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nw = mask.shape[0]\n",
    "            attn = attn.view(b_ // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, n, n)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b_, n, c)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HAB(nn.Module):\n",
    "    r\"\"\" Hybrid Attention Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 input_resolution,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 shift_size=0,\n",
    "                 compress_ratio=3,\n",
    "                 squeeze_factor=30,\n",
    "                 conv_scale=0.01,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, 'shift_size must in 0-window_size'\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop)\n",
    "\n",
    "        self.conv_scale = conv_scale\n",
    "        self.conv_block = CAB(num_feat=dim, compress_ratio=compress_ratio, squeeze_factor=squeeze_factor)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, x_size, rpi_sa, attn_mask):\n",
    "        h, w = x_size\n",
    "        b, _, c = x.shape\n",
    "        # assert seq_len == h * w, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(b, h, w, c)\n",
    "\n",
    "        # Conv_X\n",
    "        conv_x = self.conv_block(x.permute(0, 3, 1, 2))\n",
    "        conv_x = conv_x.permute(0, 2, 3, 1).contiguous().view(b, h * w, c)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = attn_mask\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nw*b, window_size, window_size, c\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, c)  # nw*b, window_size*window_size, c\n",
    "\n",
    "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
    "        attn_windows = self.attn(x_windows, rpi=rpi_sa, mask=attn_mask)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, c)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, h, w)  # b h' w' c\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            attn_x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            attn_x = shifted_x\n",
    "        attn_x = attn_x.view(b, h * w, c)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(attn_x) + conv_x * self.conv_scale\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: b, h*w, c\n",
    "        \"\"\"\n",
    "        h, w = self.input_resolution\n",
    "        b, seq_len, c = x.shape\n",
    "        assert seq_len == h * w, 'input feature has wrong size'\n",
    "        assert h % 2 == 0 and w % 2 == 0, f'x size ({h}*{w}) are not even.'\n",
    "\n",
    "        x = x.view(b, h, w, c)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # b h/2 w/2 c\n",
    "        x1 = x[:, 1::2, 0::2, :]  # b h/2 w/2 c\n",
    "        x2 = x[:, 0::2, 1::2, :]  # b h/2 w/2 c\n",
    "        x3 = x[:, 1::2, 1::2, :]  # b h/2 w/2 c\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # b h/2 w/2 4*c\n",
    "        x = x.view(b, -1, 4 * c)  # b h/2*w/2 4*c\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class OCAB(nn.Module):\n",
    "    # overlapping cross-attention block\n",
    "\n",
    "    def __init__(self, dim,\n",
    "                input_resolution,\n",
    "                window_size,\n",
    "                overlap_ratio,\n",
    "                num_heads,\n",
    "                qkv_bias=True,\n",
    "                qk_scale=None,\n",
    "                mlp_ratio=2,\n",
    "                norm_layer=nn.LayerNorm\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        #print(dim)\n",
    "        self.input_resolution = input_resolution\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "        self.overlap_win_size = int(window_size * overlap_ratio) + window_size\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.qkv = nn.Linear(dim, dim * 3,  bias=qkv_bias)\n",
    "        self.unfold = nn.Unfold(kernel_size=(self.overlap_win_size, self.overlap_win_size), stride=window_size, padding=(self.overlap_win_size-window_size)//2)\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((window_size + self.overlap_win_size - 1) * (window_size + self.overlap_win_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.proj = nn.Linear(dim,dim)\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU)\n",
    "\n",
    "    def forward(self, x, x_size, rpi):\n",
    "        h, w = x_size\n",
    "        b, _, c = x.shape\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(b, h, w, c)\n",
    "\n",
    "        qkv = self.qkv(x).reshape(b, h, w, 3, c).permute(3, 0, 4, 1, 2) # 3, b, c, h, w\n",
    "        q = qkv[0].permute(0, 2, 3, 1) # b, h, w, c\n",
    "        print('value of q is',q)\n",
    "        kv = torch.cat((qkv[1], qkv[2]), dim=1) # b, 2*c, h, w\n",
    "\n",
    "        # partition windows\n",
    "        q_windows = window_partition(q, self.window_size)  # nw*b, window_size, window_size, c\n",
    "        q_windows = q_windows.view(-1, self.window_size * self.window_size, c)  # nw*b, window_size*window_size, c\n",
    "\n",
    "        kv_windows = self.unfold(kv) # b, c*w*w, nw\n",
    "        \n",
    "        kv_windows = rearrange(kv_windows, 'b (nc ch owh oww) nw -> nc (b nw) (owh oww) ch', nc=2, ch=c, owh=self.overlap_win_size, oww=self.overlap_win_size).contiguous() # 2, nw*b, ow*ow, c\n",
    "        k_windows, v_windows = kv_windows[0], kv_windows[1] # nw*b, ow*ow, c\n",
    "        #print('kv_windows',kv_windows.shape)\n",
    "        #print('k_windows shape',k_windows.shape)\n",
    "        b_, nq, _ = q_windows.shape\n",
    "        _, n, _ = k_windows.shape\n",
    "        #print(k_windows.shape)\n",
    "        d = self.dim // self.num_heads\n",
    "        q = q_windows.reshape(b_, nq, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, nq, d\n",
    "        k = k_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, n, d\n",
    "        v = v_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, n, d\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(\n",
    "            self.window_size * self.window_size, self.overlap_win_size * self.overlap_win_size, -1)  # ws*ws, wse*wse, nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, ws*ws, wse*wse\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn_windows = (attn @ v).transpose(1, 2).reshape(b_, nq, self.dim)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, self.dim)\n",
    "        x = window_reverse(attn_windows, self.window_size, h, w)  # b h w c\n",
    "        x = x.view(b, h * w, self.dim)\n",
    "\n",
    "        x = self.proj(x) + shortcut\n",
    "\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttenBlocks(nn.Module):\n",
    "    \"\"\" A series of attention blocks for one RHAG.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 input_resolution,\n",
    "                 depth=2,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 compress_ratio,\n",
    "                 squeeze_factor,\n",
    "                 conv_scale,\n",
    "                 overlap_ratio,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            HAB(\n",
    "                dim=dim,\n",
    "                input_resolution=input_resolution,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                compress_ratio=compress_ratio,\n",
    "                squeeze_factor=squeeze_factor,\n",
    "                conv_scale=conv_scale,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer) for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        # OCAB\n",
    "        self.overlap_attn = OCAB(\n",
    "                            dim=dim,\n",
    "                            input_resolution=input_resolution,\n",
    "                            window_size=window_size,\n",
    "                            overlap_ratio=overlap_ratio,\n",
    "                            num_heads=num_heads,\n",
    "                            qkv_bias=qkv_bias,\n",
    "                            qk_scale=qk_scale,\n",
    "                            mlp_ratio=mlp_ratio,\n",
    "                            norm_layer=norm_layer\n",
    "                            )\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, x_size, params):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, x_size, params['rpi_sa'], params['attn_mask'])\n",
    "\n",
    "        x = self.overlap_attn(x, x_size, params['rpi_oca'])\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RHAG(nn.Module):\n",
    "    \"\"\"Residual Hybrid Attention Group (RHAG).\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        img_size: Input image size.\n",
    "        patch_size: Patch size.\n",
    "        resi_connection: The convolutional block before residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 input_resolution,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 compress_ratio,\n",
    "                 squeeze_factor,\n",
    "                 conv_scale,\n",
    "                 overlap_ratio,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False,\n",
    "                 img_size=224,\n",
    "                 patch_size=4,\n",
    "                 resi_connection='1conv'):\n",
    "        super(RHAG, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        self.residual_group = AttenBlocks(\n",
    "            dim=dim,\n",
    "            input_resolution=input_resolution,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            compress_ratio=compress_ratio,\n",
    "            squeeze_factor=squeeze_factor,\n",
    "            conv_scale=conv_scale,\n",
    "            overlap_ratio=overlap_ratio,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            drop=drop,\n",
    "            attn_drop=attn_drop,\n",
    "            drop_path=drop_path,\n",
    "            norm_layer=norm_layer,\n",
    "            downsample=downsample,\n",
    "            use_checkpoint=use_checkpoint)\n",
    "\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "        elif resi_connection == 'identity':\n",
    "            self.conv = nn.Identity()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim, norm_layer=None)\n",
    "\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim, norm_layer=None)\n",
    "\n",
    "    def forward(self, x, x_size, params):\n",
    "        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size, params), x_size))) + x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)  # b Ph*Pw c\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Unembedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], self.embed_dim, x_size[0], x_size[1])  # b Ph*Pw c\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"Upsample module.\n",
    "\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "        elif scale == 3:\n",
    "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "        else:\n",
    "            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n",
    "        super(Upsample, self).__init__(*m)\n",
    "\n",
    "\n",
    "#@ARCH_REGISTRY.register()\n",
    "class HAT(nn.Module):\n",
    "    r\"\"\" Hybrid Attention Transformer\n",
    "        A PyTorch implementation of : `Activating More Pixels in Image Super-Resolution Transformer`.\n",
    "        Some codes are based on SwinIR.\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 64\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 1\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n",
    "        img_range: Image range. 1. or 255.\n",
    "        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n",
    "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 img_size=280,\n",
    "                 \n",
    "                 patch_size=1,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=98,\n",
    "                 depths=(6, 6, 6, 6),\n",
    "                 num_heads=(7,7,7,7),\n",
    "                 window_size=7,\n",
    "                 compress_ratio=3,\n",
    "                 squeeze_factor=30,\n",
    "                 conv_scale=0.01,\n",
    "                 overlap_ratio=0.5,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 ape=False,\n",
    "                 patch_norm=True,\n",
    "                 use_checkpoint=False,\n",
    "                 upscale=4,\n",
    "                 img_range=1.,\n",
    "                 upsampler='pixelshuffle',\n",
    "                 resi_connection='1conv',\n",
    "                 **kwargs):\n",
    "        super(HAT, self).__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "\n",
    "        num_in_ch = in_chans\n",
    "        num_out_ch = in_chans\n",
    "        num_feat = 64\n",
    "        self.img_range = img_range\n",
    "        if in_chans == 3:\n",
    "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n",
    "        else:\n",
    "            self.mean = torch.zeros(1, 1, 1, 1)\n",
    "        self.upscale = upscale\n",
    "        self.upsampler = upsampler\n",
    "\n",
    "        # relative position index\n",
    "        relative_position_index_SA = self.calculate_rpi_sa()\n",
    "        relative_position_index_OCA = self.calculate_rpi_oca()\n",
    "        self.register_buffer('relative_position_index_SA', relative_position_index_SA)\n",
    "        self.register_buffer('relative_position_index_OCA', relative_position_index_OCA)\n",
    "\n",
    "        # ------------------------- 1, shallow feature extraction ------------------------- #\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n",
    "\n",
    "        # ------------------------- 2, deep feature extraction ------------------------- #\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=embed_dim,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # merge non-overlapping patches into image\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=embed_dim,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build Residual Hybrid Attention Groups (RHAG)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = RHAG(\n",
    "                dim=embed_dim,\n",
    "                input_resolution=(patches_resolution[0], patches_resolution[1]),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                compress_ratio=compress_ratio,\n",
    "                squeeze_factor=squeeze_factor,\n",
    "                conv_scale=conv_scale,\n",
    "                overlap_ratio=overlap_ratio,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                img_size=img_size,\n",
    "                patch_size=patch_size,\n",
    "                resi_connection=resi_connection)\n",
    "            self.layers.append(layer)\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "\n",
    "        # build the last conv layer in deep feature extraction\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
    "        elif resi_connection == 'identity':\n",
    "            self.conv_after_body = nn.Identity()\n",
    "\n",
    "        # ------------------------- 3, high quality image reconstruction ------------------------- #\n",
    "        if self.upsampler == 'pixelshuffle':\n",
    "            # for classical SR\n",
    "            self.conv_before_upsample = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True))\n",
    "            self.upsample = Upsample(upscale, num_feat)\n",
    "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def calculate_rpi_sa(self):\n",
    "        # calculate relative position index for SA\n",
    "        coords_h = torch.arange(self.window_size)\n",
    "        coords_w = torch.arange(self.window_size)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        return relative_position_index\n",
    "\n",
    "    def calculate_rpi_oca(self):\n",
    "        # calculate relative position index for OCA\n",
    "        window_size_ori = self.window_size\n",
    "        window_size_ext = self.window_size + int(self.overlap_ratio * self.window_size)\n",
    "\n",
    "        coords_h = torch.arange(window_size_ori)\n",
    "        coords_w = torch.arange(window_size_ori)\n",
    "        coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, ws, ws\n",
    "        coords_ori_flatten = torch.flatten(coords_ori, 1)  # 2, ws*ws\n",
    "\n",
    "        coords_h = torch.arange(window_size_ext)\n",
    "        coords_w = torch.arange(window_size_ext)\n",
    "        coords_ext = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, wse, wse\n",
    "        coords_ext_flatten = torch.flatten(coords_ext, 1)  # 2, wse*wse\n",
    "\n",
    "        relative_coords = coords_ext_flatten[:, None, :] - coords_ori_flatten[:, :, None]   # 2, ws*ws, wse*wse\n",
    "\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # ws*ws, wse*wse, 2\n",
    "        relative_coords[:, :, 0] += window_size_ori - window_size_ext + 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += window_size_ori - window_size_ext + 1\n",
    "\n",
    "        relative_coords[:, :, 0] *= window_size_ori + window_size_ext - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        return relative_position_index\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        # calculate attention mask for SW-MSA\n",
    "        h, w = x_size\n",
    "        img_mask = torch.zeros((1, h, w, 1))  # 1 h w 1\n",
    "        h_slices = (slice(0, -self.window_size), slice(-self.window_size,\n",
    "                                                       -self.shift_size), slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size), slice(-self.window_size,\n",
    "                                                       -self.shift_size), slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nw, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x_size = (x.shape[2], x.shape[3])\n",
    "\n",
    "        # Calculate attention mask and relative position index in advance to speed up inference. \n",
    "        # The original code is very time-consuming for large window size.\n",
    "        attn_mask = self.calculate_mask(x_size).to(x.device)\n",
    "        params = {'attn_mask': attn_mask, 'rpi_sa': self.relative_position_index_SA, 'rpi_oca': self.relative_position_index_OCA}\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x_size, params)\n",
    "\n",
    "        x = self.norm(x)  # b seq_len c\n",
    "        x = self.patch_unembed(x, x_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mean = self.mean.type_as(x)\n",
    "        x = (x - self.mean) * self.img_range\n",
    "\n",
    "        if self.upsampler == 'pixelshuffle':\n",
    "            # for classical SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "            x = self.conv_last(self.upsample(x))\n",
    "\n",
    "        x = x / self.img_range + self.mean\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VGG_Block(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, kernel_size=3,norm_type='batch', act_type='leakyrelu'):\n",
    "        super(VGG_Block, self).__init__()\n",
    "\n",
    "        self.conv0 = conv_block(in_nc, out_nc, kernel_size=kernel_size, norm_type=norm_type, act_type=act_type)\n",
    "        self.conv1 = conv_block(out_nc, out_nc, kernel_size=kernel_size, stride=2, norm_type=None,act_type=act_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv0(x)\n",
    "        out = self.conv1(x1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class VGGGAPQualifier(nn.Module):\n",
    "    def __init__(self, in_nc=3, base_nf=32, norm_type='batch', act_type='leakyrelu', mode='CNA'):\n",
    "        super(VGGGAPQualifier, self).__init__()\n",
    "        # 1024,768,3\n",
    "\n",
    "        B11 = VGG_Block(in_nc,base_nf,norm_type=norm_type,act_type=act_type)\n",
    "        # 512,384,32\n",
    "        B12 = VGG_Block(base_nf,base_nf,norm_type=norm_type,act_type=act_type)\n",
    "        # 256,192,32\n",
    "        B13 = VGG_Block(base_nf,base_nf*2,norm_type=norm_type,act_type=act_type)\n",
    "        # 128,96,64\n",
    "        B14 = VGG_Block(base_nf*2,base_nf*2,norm_type=norm_type,act_type=act_type)\n",
    "        # 64,48,64\n",
    "\n",
    "        # 1024,768,3\n",
    "        B21 = VGG_Block(in_nc,base_nf,norm_type=norm_type,act_type=act_type)\n",
    "        # 512,384,32\n",
    "        B22 = VGG_Block(base_nf,base_nf,norm_type=norm_type,act_type=act_type)\n",
    "        # 256,192,32\n",
    "        B23 = VGG_Block(base_nf,base_nf*2,norm_type=norm_type,act_type=act_type)\n",
    "        # 128,96,64\n",
    "        B24 = VGG_Block(base_nf*2,base_nf*2,norm_type=norm_type,act_type=act_type)\n",
    "        # 64,48,64\n",
    "\n",
    "\n",
    "        B3 = VGG_Block(base_nf*2,base_nf*4,norm_type=norm_type,act_type=act_type)\n",
    "        # 32,24,128\n",
    "        B4 = VGG_Block(base_nf*4,base_nf*8,norm_type=norm_type,act_type=act_type)\n",
    "        # 16,12,256\n",
    "        B5 = VGG_Block(base_nf*8,base_nf*16,norm_type=norm_type,act_type=act_type)\n",
    "        \n",
    "        self.feature1 = sequential(B11,B12,B13,B14)\n",
    "        self.feature2 = sequential(B21,B22,B23,B24)\n",
    "\n",
    "        self.combine = sequential(B3,B4,B5)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # classifie\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(base_nf*16, 512), nn.LeakyReLU(0.2, True), nn.Dropout(0.25), nn.Linear(512,256),nn.LeakyReLU(0.2, True), nn.Dropout(0.5), nn.Linear(256, 1), nn.LeakyReLU(0.2, True))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        f1 = self.feature1(x)\n",
    "        f2 = self.feature2(x)\n",
    "        x = self.gap(self.combine(f1-f2))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    '''\n",
    "    ResNet Block, 3-3 style\n",
    "    with extra residual scaling used in EDSR\n",
    "    (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW 17)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_nc, mid_nc, out_nc, kernel_size=3, stride=1, dilation=1, groups=1, \\\n",
    "            bias=True, pad_type='zero', norm_type=None, act_type='relu', mode='CNA', res_scale=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        conv0 = conv_block(in_nc, mid_nc, kernel_size, stride, dilation, groups, bias, pad_type, \\\n",
    "            norm_type, act_type, mode)\n",
    "        if mode == 'CNA':\n",
    "            act_type = None\n",
    "        if mode == 'CNAC':  # Residual path: |-CNAC-|\n",
    "            act_type = None\n",
    "            norm_type = None\n",
    "        conv1 = conv_block(mid_nc, out_nc, kernel_size, stride, dilation, groups, bias, pad_type, \\\n",
    "            norm_type, act_type, mode)\n",
    "        # if in_nc != out_nc:\n",
    "        #     self.project = conv_block(in_nc, out_nc, 1, stride, dilation, 1, bias, pad_type, \\\n",
    "        #         None, None)\n",
    "        #     print('Need a projecter in ResNetBlock.')\n",
    "        # else:\n",
    "        #     self.project = lambda x:x\n",
    "        self.res = sequential(conv0, conv1)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.res(x).mul(self.res_scale)\n",
    "        return x + res\n",
    "\n",
    "class ResidualDenseBlock_5C(nn.Module):\n",
    "    '''\n",
    "    Residual Dense Block\n",
    "    style: 5 convs\n",
    "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nc, kernel_size=3, gc=32, stride=1, bias=True, pad_type='zero', \\\n",
    "            norm_type=None, act_type='leakyrelu', mode='CNA'):\n",
    "        super(ResidualDenseBlock_5C, self).__init__()\n",
    "        # gc: growth channel, i.e. intermediate channels\n",
    "        self.conv1 = conv_block(nc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
    "            norm_type=norm_type, act_type=act_type, mode=mode)\n",
    "        self.conv2 = conv_block(nc+gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
    "            norm_type=norm_type, act_type=act_type, mode=mode)\n",
    "        self.conv3 = conv_block(nc+2*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
    "            norm_type=norm_type, act_type=act_type, mode=mode)\n",
    "        self.conv4 = conv_block(nc+3*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type, \\\n",
    "            norm_type=norm_type, act_type=act_type, mode=mode)\n",
    "        if mode == 'CNA':\n",
    "            last_act = None\n",
    "        else:\n",
    "            last_act = act_type\n",
    "        self.conv5 = conv_block(nc+4*gc, nc, 3, stride, bias=bias, pad_type=pad_type, \\\n",
    "            norm_type=norm_type, act_type=last_act, mode=mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(torch.cat((x, x1), 1))\n",
    "        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n",
    "        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5.mul(0.2) + x\n",
    "\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    '''\n",
    "    Residual in Residual Dense Block\n",
    "    (ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nc, kernel_size=3, gc=32, stride=1, bias=True, pad_type='zero', \\\n",
    "            norm_type=None, act_type='leakyrelu', mode='CNA'):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.RDB1 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n",
    "            norm_type, act_type, mode)\n",
    "        self.RDB2 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n",
    "            norm_type, act_type, mode)\n",
    "        self.RDB3 = ResidualDenseBlock_5C(nc, kernel_size, gc, stride, bias, pad_type, \\\n",
    "            norm_type, act_type, mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.RDB1(x)\n",
    "        out = self.RDB2(out)\n",
    "        out = self.RDB3(out)\n",
    "        return out.mul(0.2) + x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "# Upsampler\n",
    "####################\n",
    "\n",
    "\n",
    "def pixelshuffle_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
    "                        pad_type='zero', norm_type=None, act_type='leakyrelu'):\n",
    "    '''\n",
    "    Pixel shuffle layer\n",
    "    (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional\n",
    "    Neural Network, CVPR17)\n",
    "    '''\n",
    "    conv = conv_block(in_nc, out_nc * (upscale_factor ** 2), kernel_size, stride, bias=bias, \\\n",
    "                        pad_type=pad_type, norm_type=None, act_type=None)\n",
    "    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "    n = norm(norm_type, out_nc) if norm_type else None\n",
    "    a = act(act_type) if act_type else None\n",
    "    return sequential(conv, pixel_shuffle, n, a)\n",
    "\n",
    "\n",
    "def upconv_blcok(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
    "                pad_type='zero', norm_type=None, act_type='leakyrelu', mode='nearest'):\n",
    "    # Up conv\n",
    "    # described in https://distill.pub/2016/deconv-checkerboard/\n",
    "    upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)\n",
    "    conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias, \\\n",
    "                        pad_type=pad_type, norm_type=norm_type, act_type=act_type)\n",
    "    return sequential(upsample, conv)\n",
    "\n",
    "def downconv_blcok(in_nc, out_nc, downscale_factor=2, kernel_size=3, stride=1, bias=True, \\\n",
    "                pad_type='zero', norm_type=None, act_type='leakyrelu', mode='nearest'):\n",
    "    # Up conv\n",
    "    # described in https://distill.pub/2016/deconv-checkerboard/\n",
    "    f = 0.5\n",
    "    upsample = nn.Upsample(scale_factor=f)\n",
    "    conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias, \\\n",
    "                        pad_type=pad_type, norm_type=norm_type, act_type=act_type)\n",
    "    return sequential(upsample, conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b627f12f-3b99-48fe-83cd-6042903894b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=HAT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1fff909a-cc9c-49a9-9fe4-15dba9885c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1600, 49, 98])\n",
      "torch.Size([1600, 49, 98])\n",
      "torch.Size([1600, 49, 98])\n",
      "torch.Size([1600, 49, 98])\n",
      "torch.Size([1600, 49, 98])\n",
      "torch.Size([1600, 49, 98])\n",
      "value of q is tensor([[[[ 3.2895e-01,  5.1826e-02, -5.8472e-02,  ...,  2.4882e-02,\n",
      "           -9.7424e-02, -9.7261e-02],\n",
      "          [ 2.5602e-01,  2.9389e-01, -4.6965e-01,  ...,  1.9261e-01,\n",
      "           -2.3513e-01,  5.3828e-02],\n",
      "          [ 3.2600e-01,  4.2424e-01, -5.1538e-01,  ..., -4.4570e-02,\n",
      "            1.2329e-01, -1.0036e-01],\n",
      "          ...,\n",
      "          [ 8.2641e-02,  3.9631e-01,  8.5645e-02,  ..., -1.6214e-02,\n",
      "            4.0853e-03,  1.0953e-01],\n",
      "          [ 1.0524e-01,  2.3510e-01, -3.3699e-01,  ...,  1.4988e-01,\n",
      "           -5.8484e-02,  1.0610e-01],\n",
      "          [-4.2125e-02,  2.0804e-01, -2.4123e-01,  ..., -4.4353e-02,\n",
      "            1.1994e-01, -8.4807e-02]],\n",
      "\n",
      "         [[ 2.2848e-01,  2.0159e-01, -2.6103e-01,  ...,  5.8390e-02,\n",
      "            4.4439e-02, -1.6401e-01],\n",
      "          [ 3.4776e-03,  9.8472e-02, -4.0877e-01,  ...,  3.2969e-02,\n",
      "            1.9895e-01, -3.6571e-01],\n",
      "          [ 3.2149e-01, -8.5514e-02, -4.7989e-02,  ...,  8.3511e-02,\n",
      "            1.4532e-01,  1.1015e-01],\n",
      "          ...,\n",
      "          [ 2.7677e-02,  3.2027e-01, -5.4420e-03,  ..., -1.9107e-01,\n",
      "           -6.9679e-02,  1.4298e-01],\n",
      "          [ 1.3275e-02,  1.2122e-01, -2.1104e-01,  ...,  1.2745e-01,\n",
      "            2.2550e-01, -3.7267e-02],\n",
      "          [ 3.2651e-01,  2.8186e-01, -2.0143e-01,  ...,  1.8642e-01,\n",
      "            5.0723e-02,  4.0534e-02]],\n",
      "\n",
      "         [[ 8.4931e-02,  3.0680e-01, -4.3649e-01,  ...,  2.8576e-01,\n",
      "            1.1191e-01,  1.6168e-01],\n",
      "          [ 2.4424e-02, -1.7595e-02, -2.4699e-01,  ...,  5.9657e-02,\n",
      "            4.0568e-02, -4.3970e-04],\n",
      "          [-1.0864e-01,  2.0922e-01,  5.5258e-02,  ...,  1.5364e-01,\n",
      "            2.0791e-01,  8.7806e-02],\n",
      "          ...,\n",
      "          [-2.4004e-02,  8.7813e-02, -7.6371e-02,  ..., -1.0219e-01,\n",
      "           -2.4599e-01,  5.6327e-02],\n",
      "          [ 1.5331e-01,  8.8344e-02, -1.3192e-01,  ..., -7.2462e-02,\n",
      "            5.6504e-02, -3.2150e-01],\n",
      "          [ 1.0168e-01,  2.2460e-01, -1.1755e-01,  ..., -3.7103e-02,\n",
      "            1.7291e-02, -1.3397e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2449e-01,  5.1521e-01, -3.8815e-01,  ...,  6.2494e-02,\n",
      "            7.6654e-02, -2.1753e-02],\n",
      "          [ 8.3920e-02,  1.4722e-01, -3.1228e-01,  ..., -2.5710e-02,\n",
      "           -5.5743e-02, -7.4745e-02],\n",
      "          [ 5.7654e-03,  3.7457e-01, -1.4812e-01,  ..., -1.7605e-01,\n",
      "            5.1620e-02,  3.1213e-01],\n",
      "          ...,\n",
      "          [ 7.8069e-02,  5.3978e-01, -2.4890e-01,  ...,  4.2489e-02,\n",
      "            2.1271e-01,  1.7815e-01],\n",
      "          [-1.4581e-01, -2.4786e-02, -1.8070e-01,  ..., -1.0457e-01,\n",
      "            4.6499e-02,  6.5583e-02],\n",
      "          [-4.9915e-02,  1.3247e-01, -3.4319e-02,  ...,  2.0605e-01,\n",
      "            1.8792e-01,  2.5201e-01]],\n",
      "\n",
      "         [[ 1.3397e-01,  6.6946e-02, -6.3255e-02,  ...,  1.0053e-01,\n",
      "            1.0501e-01,  2.4305e-02],\n",
      "          [ 2.4567e-01,  7.4669e-02, -2.1780e-01,  ...,  1.2684e-01,\n",
      "            1.1743e-01, -1.5337e-01],\n",
      "          [ 3.0400e-01, -1.6334e-01, -2.1735e-01,  ..., -5.1221e-02,\n",
      "            1.5032e-01, -7.3018e-02],\n",
      "          ...,\n",
      "          [-8.1486e-02,  2.9640e-01,  5.3051e-02,  ...,  4.2412e-02,\n",
      "           -2.0484e-02,  3.5422e-01],\n",
      "          [ 1.8516e-02,  1.3160e-01, -2.5713e-02,  ...,  7.8607e-02,\n",
      "            1.1058e-01,  8.3267e-02],\n",
      "          [-6.0016e-03,  1.2895e-01, -2.6100e-02,  ...,  8.3507e-03,\n",
      "           -7.0548e-02,  6.0422e-02]],\n",
      "\n",
      "         [[ 1.9794e-01,  2.5991e-01, -1.5155e-01,  ...,  1.0813e-01,\n",
      "            1.9224e-01,  1.6365e-03],\n",
      "          [-2.8516e-01,  4.2711e-01, -9.7343e-02,  ...,  7.2080e-02,\n",
      "           -8.3306e-02,  2.1390e-01],\n",
      "          [ 1.2766e-01,  2.2284e-01,  2.1517e-01,  ...,  1.4640e-01,\n",
      "            3.6254e-02,  2.3710e-01],\n",
      "          ...,\n",
      "          [-5.0212e-03,  5.0176e-01,  1.4543e-01,  ...,  1.4926e-01,\n",
      "            1.6392e-01,  1.3575e-01],\n",
      "          [ 8.3529e-02,  3.8102e-01,  6.0271e-02,  ...,  1.9487e-01,\n",
      "           -1.2018e-01,  2.7092e-01],\n",
      "          [ 2.2743e-01,  1.5832e-01, -1.0918e-01,  ...,  1.5858e-01,\n",
      "            2.6413e-02, -3.1235e-03]]]], grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1600, 100, 7, 14]' is invalid for input of size 14905800",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m280\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m280\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[96], line 1609\u001b[0m, in \u001b[0;36mHAT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsampler \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixelshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1607\u001b[0m     \u001b[38;5;66;03m# for classical SR\u001b[39;00m\n\u001b[0;32m   1608\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_first(x)\n\u001b[1;32m-> 1609\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_after_body(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m   1610\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_before_upsample(x)\n\u001b[0;32m   1611\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_last(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(x))\n",
      "Cell \u001b[1;32mIn[96], line 1595\u001b[0m, in \u001b[0;36mHAT.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1592\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m-> 1595\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)  \u001b[38;5;66;03m# b seq_len c\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_unembed(x, x_size)\n",
      "File \u001b[1;32mD:\\Pytorch\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[96], line 1249\u001b[0m, in \u001b[0;36mRHAG.forward\u001b[1;34m(self, x, x_size, params)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_size, params):\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_unembed(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m, x_size))) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[1;32mD:\\Pytorch\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[96], line 1160\u001b[0m, in \u001b[0;36mAttenBlocks.forward\u001b[1;34m(self, x, x_size, params)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m   1158\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x, x_size, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrpi_sa\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 1160\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrpi_oca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1163\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32mD:\\Pytorch\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[96], line 1046\u001b[0m, in \u001b[0;36mOCAB.forward\u001b[1;34m(self, x, x_size, rpi)\u001b[0m\n\u001b[0;32m   1044\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[0;32m   1045\u001b[0m q \u001b[38;5;241m=\u001b[39m q_windows\u001b[38;5;241m.\u001b[39mreshape(b_, nq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, d)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# nw*b, nH, nq, d\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mk_windows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# nw*b, nH, n, d\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m v \u001b[38;5;241m=\u001b[39m v_windows\u001b[38;5;241m.\u001b[39mreshape(b_, n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, d)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# nw*b, nH, n, d\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1600, 100, 7, 14]' is invalid for input of size 14905800"
     ]
    }
   ],
   "source": [
    "model.forward(torch.rand(1,3,280,280)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532ea24-2bbb-429f-a1f7-0ae7eb104b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d4b2e-42d1-46cf-b38e-234e179a9682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c59e30-67b5-4b00-822c-c5923cc740e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
